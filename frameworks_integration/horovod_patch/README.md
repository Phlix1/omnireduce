# Frameworks Integration
By changing a few lines of code in Horovod we are able to delegate allreduce SUM operations to OmniReduce.

We introduce OminReduce into Horovod. In details, such as NCCLAllreduce, we implement OmniAllreduce's Execute function which invokes the OmniContext Allreduce op. The users who want to use omnireduce in Horovod only set `HOROVOD_GPU_ALLREDUCE=OMNI` to compile Horovod. It is nothing to change in user's code. 

For OmniReduce to take over from Horovod the following conditions must be met:
- The all reduce operation must be a summation
- The data type must be float or int32
- Each node/host produces 1 tensor or in other words each node/host uses 1 GPU.

## 1. Download Horovod
The patch currently applies to v0.19.4 branch in Horovod.
  
    git clone --branch v0.19.4 https://github.com/horovod/horovod.git
    cd horovod
    git submodule sync
    git submodule update --init --recursive 

This will also take a good while to clone and checkout all submodules.

## 2. Download OmniReduce
Download the Omnireduce and put it into the `horovod/third_party` content.

    git clone https://github.com/sands-lab/omnireduce.git
    mv omnireduce horovod/third_party

## 3. Install Boost C++ library
The version of Boost C++ library is `1.65.1`.
For Ubuntu, it is simple to install it by:

    apt-get install -y libboost-all-dev=1.65.1.0ubuntu1

For CentOS, we firstly download boost in [boost.org](https://www.boost.org/users/history/version_1_65_1.html) and then using to `gcc>=5.3.1` to complie boost from source:

    tar zxvf boost_1_65_1.tar.gz
    cd boost_1_65_1
    ./bootstrap.sh
    ./b2 install --with=all

## 4. Install OmniReduce
Build OmniReduce and copy the omnireduce `dynami library` and `header files` in the `build` folder to the system `library` and `include` path.

    cd horovod/third_party/omnireduce/omnireduce-RDMA
    make USE_CUDA=ON
    cp ./build/libomnireduce.so SYSTEM_LIBRARY_PATH
    cp -r ./build/include/omnireduce SYSTEM_INCLUDE_PATH
    cd horovod/third_party/omnireduce/omnireduce-RDMA/example
    make USE_CUDA=ON
    
## 5. Apply patch to Horovod

    cd horovod
    git apply omnireduce-horovod.patch
    
## 6. Build Horovod with OmniReduce
Before install Horoovd, we need to install torch and tensorflow. The recommended version environment isï¼š
- **gcc version:** gcc>=5.3.1
- **Python version:** Python <= 3.7, 3.8 is not support for tensorflow1.15
- **Torch version:** Pytorch 1.6 for CUDA10.0, pytorch 1.7 for CUDA10.1 or CUDA11.0, pytorch1.8 for CUDA11.0
- **Tensorflow version:** Tensorflow == 1.15 for CUDA10.0 (tensorflow official not support CUDA11.0 for tensorflow1.x). Besides that, we currently not support tensorflow2.x.

According to the Horovod official [repository](https://github.com/horovod/horovod/tree/v0.19.4#install), we only replace `HOROVOD_GPU_ALLREDUCE=NCCL` into `HOROVOD_GPU_ALLREDUCE=OMNI` and then compiling horovod from source. We both support omnireduce for TensorFlow and PyTorch in Horovod. 

    cd horovod
    CC=`which gcc` CXX=`which g++` HOROVOD_WITHOUT_MXNET=1 HOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITH_PYTORCH=1 HOROVOD_NCCL_LINK=SHARED HOROVOD_GPU_ALLREDUCE=OMNI HOROVOD_GPU_BROADCAST=NCCL python3.6 setup.py install

## 7. Running Horovod With OmniReduce

Before running, we must to configure the `omnireduce.cfg` file. Then:
- Launch `./aggregator` in `horovod/third_party/omnireduce/omnireduce-RDMA/example` for all aggregator's machines.
- Configure the hostfile and use `mpirun` only in master-worker machine to launch Horovod. The entire command as follows:

    mpirun --hostfile ./hostfile -map-by slot --display-map --tag-output --timestamp-output --mca btl_tcp_if_exclude lo,docker0 -x NCCL_NET_GDR_READ=1 -x NCCL_IB_HCA=mlx5_0 -x NCCL_DEBUG=INFO -x NCCL_SOCKET_IFNAME=^lo,docker0 -x NCCL_IB_GID_INDEX=3 -x NCCL_IB_DISABLE=0 -x HOROVOD_MPI_THREADS_DISABLE=1 python3 test_hvd_torch.py (or test_hvd_tensorflow.py)

The hostfile of 2 machines 2 gpus is configured as followed:

    machine1_ip port=xxx max_slots=1
    machine2_ip port=xxx max_slots=1

Since each node/host uses 1 GPU, CUDA_VISIBLE_DEVICES is must be set one gpu device id. In order to GDR, we need to check the machine's topo by `nvidia-smi topo -m`and select the gpu device id which binds to the network card.

## 8. Horovod Timeline With OmniReduce
The OmniReduce implements allreduce op contains synchronize which leads to inaccurate time-consuming at each stage (`MEMCPY_IN_FUSION_BUFFER`, `OMNI_ALLREDUCE` and `MEMCPY_OUT_FUSION_BUFFER`) in omniAllreduce's Execute function. For solving this problem, we support the feature in Horovod timeline which can accurate statistics time-consuming for synchronize's op.

For example, the follow picture shows the `OMNI_ALLREDUCE` time-consuming. The red box marks the original horovod timeline time-consuming and the green box marks the time-consuming after correction.

![image](https://user-images.githubusercontent.com/25579435/125772127-b00c1518-fe44-4461-bbd3-d92879d8d050.png)

