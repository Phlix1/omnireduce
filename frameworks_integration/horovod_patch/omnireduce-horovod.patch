From d9b6ab581ddab0b6e7e39dde92c07d3580f12623 Mon Sep 17 00:00:00 2001
From: wuyuzhe02 <wuyuzhe02@meituan.com>
Date: Wed, 14 Jul 2021 17:23:48 +0800
Subject: [PATCH] Support omniAllreduce in horovod

---
 horovod/common/common.h                     |   3 +
 horovod/common/operations.cc                |  11 +
 horovod/common/ops/cuda_operations.cc       |  43 +++-
 horovod/common/ops/gpu_context_impl.cc      |   4 +-
 horovod/common/ops/gpu_operations.cc        |   5 +-
 horovod/common/ops/gpu_operations.h         |   3 +-
 horovod/common/ops/omnireduce_operations.cc | 113 ++++++++++
 horovod/common/ops/omnireduce_operations.h  |  34 +++
 horovod/common/timeline.cc                  |  10 +-
 horovod/common/timeline.h                   |   6 +-
 setup.py                                    |  30 +--
 test/test_tensorflow_omni.py                | 231 +++++++++++++++++++
 test/test_torch_omni.py                     | 238 ++++++++++++++++++++
 13 files changed, 703 insertions(+), 28 deletions(-)
 create mode 100644 horovod/common/ops/omnireduce_operations.cc
 create mode 100644 horovod/common/ops/omnireduce_operations.h
 create mode 100644 test/test_tensorflow_omni.py
 create mode 100644 test/test_torch_omni.py

diff --git a/horovod/common/common.h b/horovod/common/common.h
index d469b69..5c9bc2e 100644
--- a/horovod/common/common.h
+++ b/horovod/common/common.h
@@ -37,12 +37,15 @@ namespace common {
 #define INIT_NCCL "INIT_NCCL"
 #define QUEUE "QUEUE"
 #define MEMCPY_IN_FUSION_BUFFER "MEMCPY_IN_FUSION_BUFFER"
+#define MEMCPY_D2D "MEMCPY_D2D"
 #define MEMCPY_IN_HOST_BUFFER "MEMCPY_IN_HOST_BUFFER"
 #define MEMCPY_IN_SHARED_BUFFER "MEMCPY_IN_SHARED_BUFFER"
 #define MPI_ALLREDUCE "MPI_ALLREDUCE"
 #define MPI_ADASUM_ALLREDUCE "MPI_ADASUM_ALLREDUCE"
 #define MEMCPY_OUT_HOST_BUFFER "MEMCPY_OUT_HOST_BUFFER"
 #define NCCL_ALLREDUCE "NCCL_ALLREDUCE"
+#define OMNI_ALLREDUCE "OMNI_ALLREDUCE"
+#define OMNI_PARAM "_OMNI_PARAM_"
 #define MEMCPY_OUT_FUSION_BUFFER "MEMCPY_OUT_FUSION_BUFFER"
 #define MPI_BCAST "MPI_BCAST"
 #define NCCL_REDUCESCATTER "NCCL_REDUCESCATTER"
diff --git a/horovod/common/operations.cc b/horovod/common/operations.cc
index 3d0335b..13fff70 100644
--- a/horovod/common/operations.cc
+++ b/horovod/common/operations.cc
@@ -78,6 +78,11 @@
 #include "ops/gloo_operations.h"
 #endif
 
+#if HAVE_OMNI
+#include "omnireduce/context.hpp"
+#include "ops/omnireduce_operations.h"
+#endif
+
 /*
  * Allreduce, Allgather and Broadcast Ops.
  *
@@ -180,6 +185,12 @@ OperationManager* CreateOperationManager(HorovodGlobalState& state) {
       new NCCLAllreduce(&nccl_context, &gpu_context, &state)));
 #endif
 
+#if HAVE_OMNI && HOROVOD_GPU_ALLREDUCE == 'O'
+  omnireduce::OmniContext& omni_context = omnireduce::OmniContext::getInstance();
+  allreduce_ops.push_back(std::shared_ptr<AllreduceOp>(
+      new OmniAllreduce(&omni_context, &gpu_context, &state)));
+#endif
+
 #if HAVE_NCCL && HOROVOD_GPU_BROADCAST == 'N'
     broadcast_ops.push_back(
         std::shared_ptr<BroadcastOp>(new NCCLBroadcast(&nccl_context, &gpu_context, &state)));
diff --git a/horovod/common/ops/cuda_operations.cc b/horovod/common/ops/cuda_operations.cc
index b03f6c1..62daf2f 100644
--- a/horovod/common/ops/cuda_operations.cc
+++ b/horovod/common/ops/cuda_operations.cc
@@ -41,6 +41,11 @@ public:
       }
     }
 
+    static bool with_timeline = std::getenv(HOROVOD_TIMELINE) != nullptr;
+    if (with_timeline) {
+      return cudaEventCreateWithFlags(event, cudaEventBlockingSync);
+    }
+
     return cudaEventCreateWithFlags(event, cudaEventBlockingSync | cudaEventDisableTiming);
   }
 
@@ -75,21 +80,55 @@ public:
   }
 
   void WaitForEvents(std::queue<std::pair<std::string, cudaEvent_t>>& event_queue,
-      const std::vector<TensorTableEntry>& entries, Timeline& timeline) {
+      const std::vector<TensorTableEntry>& entries, Timeline& timeline, HorovodGlobalState* global_state) {
+    cudaEvent_t prev_event = nullptr;
+    std::string prev_param = "";
     while (!event_queue.empty()) {
       std::string name;
       cudaEvent_t event;
       std::tie(name, event) = event_queue.front();
       event_queue.pop();
       if (name != "") {
+        if (name.rfind(OMNI_PARAM, 0) == 0) {
+          if (prev_event != nullptr) {
+            ErrorCheck("ReleaseGpuEvent", ReleaseGpuEvent(prev_event));
+          }
+          prev_param = name.substr(sizeof(OMNI_PARAM) - 1);
+          prev_event = event;
+          ErrorCheck("cudaEventSynchronize", cudaEventSynchronize(prev_event));
+          continue;
+        }
         timeline.ActivityStartAll(entries, name);
       }
       ErrorCheck("cudaEventSynchronize", cudaEventSynchronize(event));
       if (name != "") {
-        timeline.ActivityEndAll(entries);
+        std::string args = "";
+        if (prev_event != nullptr) {
+          float time_ms = 0.0f;
+          auto stat = cudaEventElapsedTime(&time_ms, prev_event, event);
+          std::stringstream ss;
+          if (prev_param.size()) {
+            ss << prev_param;
+            if (cudaSuccess == stat && time_ms > 0) {
+              ss << ",\"time_ms\":" + std::to_string(time_ms);
+              size_t bytes =
+                  std::stoul(prev_param.substr(sizeof("\"bytes\":") - 1));
+              float speed = bytes / time_ms * 1e-6;
+              ss << ',' << "\"speed_gbs\":" << std::to_string(speed);
+            }
+          }
+          ErrorCheck("ReleaseGpuEvent", ReleaseGpuEvent(prev_event));
+          prev_event = nullptr;
+          prev_param = "";
+          args = ss.str();
+        }
+        timeline.ActivityEndAll(entries, args);
       }
       ErrorCheck("ReleaseGpuEvent", ReleaseGpuEvent(event));
     }
+    if (prev_event != nullptr) {
+      ErrorCheck("ReleaseGpuEvent", ReleaseGpuEvent(prev_event));
+    }
   }
 
   void StreamCreate(cudaStream_t *stream) {
diff --git a/horovod/common/ops/gpu_context_impl.cc b/horovod/common/ops/gpu_context_impl.cc
index 32e0c65..8398d17 100644
--- a/horovod/common/ops/gpu_context_impl.cc
+++ b/horovod/common/ops/gpu_context_impl.cc
@@ -13,8 +13,8 @@ void GPUContext::RecordEvent(std::queue<std::pair<std::string, gpuEvent_t>>& eve
   pimpl->RecordEvent(event_queue, name, stream);
 }
 
-void GPUContext::WaitForEvents(std::queue<std::pair<std::string, gpuEvent_t>>& event_queue, const std::vector<TensorTableEntry>& entries, Timeline& timeline) {
-  pimpl->WaitForEvents(event_queue, entries, timeline);
+void GPUContext::WaitForEvents(std::queue<std::pair<std::string, gpuEvent_t>>& event_queue, const std::vector<TensorTableEntry>& entries, Timeline& timeline, HorovodGlobalState* global_state) {
+  pimpl->WaitForEvents(event_queue, entries, timeline, global_state);
 }
 
 void GPUContext::StreamCreate(gpuStream_t *stream) {
diff --git a/horovod/common/ops/gpu_operations.cc b/horovod/common/ops/gpu_operations.cc
index 8a89f46..77704bc 100644
--- a/horovod/common/ops/gpu_operations.cc
+++ b/horovod/common/ops/gpu_operations.cc
@@ -54,6 +54,7 @@ Status GPUOpContext::FinalizeGPUQueue(const std::vector<TensorTableEntry>& entri
   auto& evt_queue = event_queue;
   auto& timeline = global_state_->timeline;
   auto& gpu_context = gpu_context_;
+  auto& global_state_gpu = global_state_;
 
   // Claim a std::shared_ptr to the fusion buffer to prevent its memory from being reclaimed
   // during finalization.
@@ -61,10 +62,10 @@ Status GPUOpContext::FinalizeGPUQueue(const std::vector<TensorTableEntry>& entri
       first_entry.device, first_entry.context->framework(), global_state_->current_nccl_stream);
 
   gpu_context_->finalizer_thread_pool.execute([entries, first_entry, cpu_buffer, fusion_buffer, free_host_buffer,
-                                                evt_queue, &timeline, &gpu_context]() mutable {
+                                                evt_queue, &timeline, &gpu_context, global_state_gpu]() mutable {
     gpu_context->SetDevice(first_entry.device);
 
-    gpu_context->WaitForEvents(evt_queue, entries, timeline);
+    gpu_context->WaitForEvents(evt_queue, entries, timeline, global_state_gpu);
     if (free_host_buffer && cpu_buffer != nullptr) {
       free(cpu_buffer);
     }
diff --git a/horovod/common/ops/gpu_operations.h b/horovod/common/ops/gpu_operations.h
index ab1b21a..6262f6f 100644
--- a/horovod/common/ops/gpu_operations.h
+++ b/horovod/common/ops/gpu_operations.h
@@ -68,7 +68,8 @@ public:
                    gpuStream_t& stream);
 
   void WaitForEvents(std::queue<std::pair<std::string, gpuEvent_t>>& event_queue,
-                     const std::vector<TensorTableEntry>& entries, Timeline& timeline);
+                     const std::vector<TensorTableEntry>& entries, Timeline& timeline,
+                     HorovodGlobalState* global_state=nullptr);
 
   void StreamCreate(gpuStream_t *stream);
   void StreamSynchronize(gpuStream_t stream);
diff --git a/horovod/common/ops/omnireduce_operations.cc b/horovod/common/ops/omnireduce_operations.cc
new file mode 100644
index 0000000..b79c08c
--- /dev/null
+++ b/horovod/common/ops/omnireduce_operations.cc
@@ -0,0 +1,113 @@
+#include "omnireduce_operations.h"
+#include "../common.h"
+#include "../global_state.h"
+#include "../logging.h"
+
+namespace horovod {
+namespace common {
+
+Status OmniAllreduce::Execute(std::vector<TensorTableEntry>& entries,
+                              const Response& response) {
+  LOG(DEBUG) << "Do GPU OmniAllreduce...";
+  auto& first_entry = entries[0];
+
+  gpu_op_context_.InitGPU(entries);
+  gpu_op_context_.InitGPUQueue(entries, response);
+
+  const void* fused_input_data;
+  void* buffer_data;
+  size_t buffer_len;
+
+  int64_t num_elements = 0;
+  for (auto& e : entries) {
+    num_elements += e.tensor->shape().num_elements();
+  }
+
+  // Copy memory into the fusion buffer.
+  if (entries.size() > 1) {
+    if (global_state_->timeline.Initialized()) {
+      gpu_context_->RecordEvent(gpu_op_context_.event_queue, 
+                                OMNI_PARAM "\"bytes\":" + std::to_string(num_elements * DataType_Size(first_entry.tensor->dtype())),
+                                *gpu_op_context_.stream);
+    }
+    MemcpyInFusionBuffer(entries, fused_input_data, buffer_data, buffer_len);
+
+    if (global_state_->timeline.Initialized()) {
+      gpu_context_->RecordEvent(gpu_op_context_.event_queue,
+                                MEMCPY_IN_FUSION_BUFFER,
+                                *gpu_op_context_.stream);
+    }
+    LOG(DEBUG) << "OmniAllreduce: MemcpyInFusionBuffer done";
+  } else {
+    if (global_state_->timeline.Initialized()) {
+      gpu_context_->RecordEvent(gpu_op_context_.event_queue, 
+                                OMNI_PARAM "\"bytes\":" + std::to_string(num_elements * DataType_Size(first_entry.tensor->dtype())),
+                                *gpu_op_context_.stream);
+    }
+    fused_input_data = first_entry.tensor->data();
+    buffer_data = (void*)first_entry.output->data();
+    gpu_context_->MemcpyAsyncD2D(buffer_data, first_entry.tensor->data(),
+                                 (size_t)first_entry.tensor->size(),
+                                 *gpu_op_context_.stream);
+    buffer_len = (size_t)first_entry.output->size();
+    if (global_state_->timeline.Initialized()) {
+      gpu_context_->RecordEvent(gpu_op_context_.event_queue,
+                                MEMCPY_D2D,
+                                *gpu_op_context_.stream);
+    }
+    LOG(DEBUG) << "OmniAllreduce: finish to Memcpy done for 1 entry";
+  }
+
+  // Do allreduce.
+  if (global_state_->timeline.Initialized()) {
+    gpu_context_->RecordEvent(gpu_op_context_.event_queue, 
+                              OMNI_PARAM "\"bytes\":" + std::to_string(num_elements * DataType_Size(first_entry.tensor->dtype())),
+                              *gpu_op_context_.stream);
+  }
+  auto dtype = first_entry.tensor->dtype();
+  switch (dtype) {
+  case HOROVOD_FLOAT32:
+    LOG(DEBUG) << "OmniAllreduce: HOROVOD_FLOAT32";
+    omniContext_->AllReduce(static_cast<float*>(buffer_data), num_elements,
+                            *gpu_op_context_.stream,
+                            global_state_->controller->GetLocalRank());
+    break;
+  case HOROVOD_INT32:
+    LOG(DEBUG) << "OmniAllreduce: HOROVOD_INT32";
+    omniContext_->AllReduce(static_cast<int32_t*>(buffer_data), num_elements,
+                            *gpu_op_context_.stream,
+                            global_state_->controller->GetLocalRank());
+    break;
+  default:
+    throw std::logic_error("omnireduce is currently only support in float32 and int32.");
+  }
+
+  LOG(DEBUG) << "OmniAllreduce: allreduce done";
+  
+  if (global_state_->timeline.Initialized()) {
+    gpu_context_->RecordEvent(gpu_op_context_.event_queue, OMNI_ALLREDUCE,
+                              *gpu_op_context_.stream);
+  }
+
+  // Copy memory out of the fusion buffer.
+  if (entries.size() > 1) {
+    if (global_state_->timeline.Initialized()) {
+      gpu_context_->RecordEvent(gpu_op_context_.event_queue, 
+                                OMNI_PARAM "\"bytes\":" + std::to_string(num_elements * DataType_Size(first_entry.tensor->dtype())),
+                                *gpu_op_context_.stream);
+    }
+    MemcpyOutFusionBuffer(buffer_data, entries);
+
+    if (global_state_->timeline.Initialized()) {
+      gpu_context_->RecordEvent(gpu_op_context_.event_queue,
+                                MEMCPY_OUT_FUSION_BUFFER,
+                                *gpu_op_context_.stream);
+    }
+    LOG(DEBUG) << "OmniAllreduce: MemcpyOutFusionBuffer done";
+  }
+
+  return gpu_op_context_.FinalizeGPUQueue(entries);
+}
+
+} // namespace common
+} // namespace horovod
diff --git a/horovod/common/ops/omnireduce_operations.h b/horovod/common/ops/omnireduce_operations.h
new file mode 100644
index 0000000..8a5502a
--- /dev/null
+++ b/horovod/common/ops/omnireduce_operations.h
@@ -0,0 +1,34 @@
+#ifndef HOROVOD_OMNIREDUCE_OPERATIONS_H
+#define HOROVOD_OMNIREDUCE_OPERATIONS_H
+#define USE_CUDA
+
+#if HAVE_MPI
+#include "../mpi/mpi_context.h"
+#endif
+
+#include "gpu_operations.h"
+#include "omnireduce/context.hpp"
+
+namespace horovod {
+namespace common {
+
+class OmniAllreduce : public GPUAllreduce {
+public:
+  OmniAllreduce(
+      omnireduce::OmniContext* omniContext, GPUContext* gpu_context,
+      HorovodGlobalState* global_state)
+      : GPUAllreduce(gpu_context, global_state), omniContext_(omniContext), 
+        global_state_(global_state){};
+
+  Status Execute(std::vector<TensorTableEntry>& entries,
+                 const Response& response) override;
+
+protected:
+  omnireduce::OmniContext* omniContext_;
+  HorovodGlobalState* global_state_;
+};
+
+} // namespace common
+} // namespace horovod
+
+#endif // HOROVOD_OMNIREDUCE_OPERATIONS_H
\ No newline at end of file
diff --git a/horovod/common/timeline.cc b/horovod/common/timeline.cc
index 4451467..d5d7a86 100644
--- a/horovod/common/timeline.cc
+++ b/horovod/common/timeline.cc
@@ -260,20 +260,22 @@ void Timeline::ActivityStart(const std::string& tensor_name,
   tensor_states_[tensor_name] = TimelineState::ACTIVITY;
 }
 
-void Timeline::ActivityEndAll(const std::vector<TensorTableEntry>& entries) {
+void Timeline::ActivityEndAll(const std::vector<TensorTableEntry>& entries,
+                              const std::string& args) {
   for (auto& e : entries) {
-    ActivityEnd(e.tensor_name);
+    ActivityEnd(e.tensor_name, args);
   }
 }
 
-void Timeline::ActivityEnd(const std::string& tensor_name) {
+void Timeline::ActivityEnd(const std::string& tensor_name,
+                           const std::string& args) {
   if (!initialized_) {
     return;
   }
 
   std::lock_guard<std::recursive_mutex> guard(mutex_);
   assert(tensor_states_[tensor_name] == TimelineState::ACTIVITY);
-  WriteEvent(tensor_name, 'E');
+  WriteEvent(tensor_name, 'E', "", args);
   tensor_states_[tensor_name] = TimelineState::TOP_LEVEL;
 }
 
diff --git a/horovod/common/timeline.h b/horovod/common/timeline.h
index 1d67819..a3015c8 100644
--- a/horovod/common/timeline.h
+++ b/horovod/common/timeline.h
@@ -92,8 +92,10 @@ public:
                         const std::string& activity);
   void ActivityStart(const std::string& tensor_name,
                      const std::string& activity);
-  void ActivityEndAll(const std::vector<TensorTableEntry>& entries);
-  void ActivityEnd(const std::string& tensor_name);
+  void ActivityEndAll(const std::vector<TensorTableEntry>& entries,
+                      const std::string& args = "");
+  void ActivityEnd(const std::string& tensor_name,
+                   const std::string& args = "");
   void End(const std::string& tensor_name, std::shared_ptr<Tensor> tensor);
   void MarkCycleStart();
 
diff --git a/setup.py b/setup.py
index ad145a7..e42e0f5 100644
--- a/setup.py
+++ b/setup.py
@@ -636,9 +636,9 @@ def get_common_options(build_ext):
 
     gpu_allreduce = os.environ.get('HOROVOD_GPU_ALLREDUCE')
     if gpu_allreduce and gpu_allreduce != 'MPI' and gpu_allreduce != 'NCCL' and \
-        gpu_allreduce != 'DDL':
+        gpu_allreduce != 'DDL' and gpu_allreduce != 'OMNI':
         raise DistutilsError('HOROVOD_GPU_ALLREDUCE=%s is invalid, supported '
-                             'values are "", "MPI", "NCCL", "DDL".' % gpu_allreduce)
+                             'values are "", "MPI", "NCCL", "DDL", "OMNI".' % gpu_allreduce)
 
     gpu_allgather = os.environ.get('HOROVOD_GPU_ALLGATHER')
     if gpu_allgather and gpu_allgather != 'MPI':
@@ -693,21 +693,13 @@ def get_common_options(build_ext):
             'If you\'re sure you want to mix them, set the '
             'HOROVOD_ALLOW_MIXED_GPU_IMPL environment variable to \'1\'.')
 
+    have_omni = False
+    if gpu_allreduce == 'OMNI':
+        print('INFO: build gpu_allreduce=omnireduce.')
+        have_omni = True
+
     MACROS = [('EIGEN_MPL2_ONLY', 1)]
     INCLUDES = ['third_party/HTTPRequest/include',
-                'third_party/boost/assert/include',
-                'third_party/boost/config/include',
-                'third_party/boost/core/include',
-                'third_party/boost/detail/include',
-                'third_party/boost/iterator/include',
-                'third_party/boost/lockfree/include',
-                'third_party/boost/mpl/include',
-                'third_party/boost/parameter/include',
-                'third_party/boost/predef/include',
-                'third_party/boost/preprocessor/include',
-                'third_party/boost/static_assert/include',
-                'third_party/boost/type_traits/include',
-                'third_party/boost/utility/include',
                 'third_party/eigen',
                 'third_party/flatbuffers/include',
                 'third_party/lbfgs/include']
@@ -776,6 +768,14 @@ def get_common_options(build_ext):
                     'horovod/common/gloo/memory_store.cc',
                     'horovod/common/ops/gloo_operations.cc']
 
+    if have_omni:
+        MACROS += [('HAVE_OMNI', '1'), ('USE_CUDA', '1')]
+        INCLUDES += ['third_party/omnireduce']
+        SOURCES += ['horovod/common/ops/omnireduce_operations.cc']
+        LIBRARIES += ['boost_chrono', 'boost_system',
+                      'boost_thread', 'boost_program_options']
+        LINK_FLAGS += ['-L%s' % './third_party/omnireduce/build', '-l%s' % 'omnireduce']
+
     if have_ccl:
         MACROS += [('HAVE_CCL', '1')]
         INCLUDES += [ccl_root + '/include/']
diff --git a/test/test_tensorflow_omni.py b/test/test_tensorflow_omni.py
new file mode 100644
index 0000000..881f607
--- /dev/null
+++ b/test/test_tensorflow_omni.py
@@ -0,0 +1,231 @@
+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
+# Modifications copyright (C) 2018 Uber Technologies, Inc.
+# Modifications copyright (C) 2019 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# =============================================================================
+
+"""Tests for horovod.tensorflow.mpi_ops."""
+
+from distutils.version import LooseVersion
+
+import itertools
+import numpy as np
+import os
+import pytest
+import tensorflow as tf
+from horovod.tensorflow.util import _executing_eagerly, _has_eager
+from tensorflow.python.framework import ops
+import warnings
+
+import horovod.tensorflow as hvd
+
+from common import mpi_env_rank_and_size
+
+if hasattr(tf, 'ConfigProto'):
+    config = tf.ConfigProto()
+    config.gpu_options.allow_growth = True
+
+if hasattr(tf, 'config') and hasattr(tf.config, 'experimental') \
+        and hasattr(tf.config.experimental, 'set_memory_growth'):
+    gpus = tf.config.experimental.list_physical_devices('GPU')
+    for gpu in gpus:
+        tf.config.experimental.set_memory_growth(gpu, True)
+else:
+    if _has_eager:
+        # Specifies the config to use with eager execution. Does not preclude
+        # tests from running in the graph mode.
+        tf.enable_eager_execution(config=config)
+
+ccl_supported_types = set([tf.uint8, tf.int32, tf.int64, tf.float32, tf.float64])
+
+_IS_TF2 = LooseVersion(tf.__version__) >= LooseVersion('2.0.0')
+
+
+class TensorFlowTests(tf.test.TestCase):
+    """
+    Tests for ops in horovod.tensorflow.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(TensorFlowTests, self).__init__(*args, **kwargs)
+        warnings.simplefilter('module')
+        if _has_eager:
+            if hasattr(tf, 'contrib') and hasattr(tf.contrib, 'eager'):
+                self.tfe = tf.contrib.eager
+            else:
+                self.tfe = tf
+
+    def evaluate(self, tensors):
+        if _executing_eagerly():
+            return self._eval_helper(tensors)
+        sess = ops.get_default_session()
+        if sess is None:
+            with self.test_session(config=config) as sess:
+                return sess.run(tensors)
+        else:
+            return sess.run(tensors)
+
+    def assign(self, variables, values):
+        if _executing_eagerly():
+            for var, val in zip(variables, values):
+                var.assign(val)
+        else:
+            sess = ops.get_default_session()
+            if sess is None:
+                with self.test_session(config=config) as sess:
+                    for var, val in zip(variables, values):
+                        var.load(val, sess)
+            else:
+                for var, val in zip(variables, values):
+                    var.load(val, sess)
+
+    def random_uniform(self, *args, **kwargs):
+        if hasattr(tf, 'random') and hasattr(tf.random, 'set_seed'):
+            tf.random.set_seed(1234)
+            return tf.random.uniform(*args, **kwargs)
+        else:
+            tf.set_random_seed(1234)
+            return tf.random_uniform(*args, **kwargs)
+
+    def filter_supported_types(self, types):
+        if 'CCL_ROOT' in os.environ:
+           types = [t for t in types if t in ccl_supported_types]
+        return types
+
+    def test_horovod_rank(self):
+        """Test that the rank returned by hvd.rank() is correct."""
+        mpi_rank, _ = mpi_env_rank_and_size()
+        gloo_rank = int(os.getenv('HOROVOD_RANK', -1))
+
+        # The mpi rank does not match gloo rank, we need to figure which one
+        # we are using to run the test.
+        is_mpi = gloo_rank == -1
+        hvd.init()
+        rank = hvd.rank()
+
+        if is_mpi:
+            assert mpi_rank == rank
+        else:
+            assert gloo_rank == rank
+
+    def test_horovod_size(self):
+        """Test that the size returned by hvd.size() is correct."""
+        _, mpi_size = mpi_env_rank_and_size()
+        gloo_size = int(os.getenv('HOROVOD_SIZE', -1))
+
+        # The mpi size does not match gloo size, we need to figure which one
+        # we are using to run the test.
+        is_mpi = gloo_size == -1
+        hvd.init()
+        size = hvd.size()
+        if is_mpi:
+            assert mpi_size == size
+        else:
+            assert gloo_size == size
+
+    def test_horovod_omni_allreduce_gpu(self):
+        """Test that the allreduce works on GPUs."""
+        # Only do this test if there are GPUs available.
+        if not tf.test.is_gpu_available(cuda_only=True):
+            self.skipTest(("No GPUs available"))
+
+        if os.environ.get('HOROVOD_MIXED_INSTALL'):
+            # Skip if compiled with CUDA but without HOROVOD_GPU_OPERATIONS.
+            self.skipTest("Not compiled with HOROVOD_GPU_OPERATIONS")
+
+        hvd.init()
+        local_rank = hvd.local_rank()
+        size = hvd.size()
+
+        dtypes = [tf.int32, tf.float32]
+        dims = [1, 2, 3]
+        #default_tensor = [-2, -1, 0, 1, 2]
+        for dtype, dim in itertools.product(dtypes, dims):
+            with tf.device("/gpu:%d" % local_rank):
+                tensor = self.random_uniform([17] * dim, -100, 100, dtype=dtype)
+                #tensor = tf.convert_to_tensor(default_tensor, dtype=dtype)
+                # print('tensor = {}'.format(self.evaluate(tensor)))
+                summed = hvd.allreduce(tensor, average=False)
+            multiplied = tensor * size
+            max_difference = tf.reduce_max(tf.abs(summed - multiplied))
+            # print('summed = {}'.format(self.evaluate(summed)))
+            # print('multiplied = {}'.format(self.evaluate(multiplied)))
+            # print('max_difference = {}'.format(self.evaluate(max_difference)))
+
+            # Threshold for floating point equality depends on number of
+            # ranks, since we're comparing against precise multiplication.
+            if size <= 3 or dtype in [tf.int32, tf.int64]:
+                threshold = 0
+            elif size < 10:
+                threshold = 1e-4
+            elif size < 15:
+                threshold = 5e-4
+            else:
+                self.skipTest("Horovod cluster too large for precise multiplication comparison")
+
+            diff = self.evaluate(max_difference)
+            self.assertTrue(diff <= threshold, "hvd.omni_allreduce on GPU produces incorrect results")
+
+    def test_horovod_omni_allreduce_gpu_fused(self):
+        """Test that the omni allreduce works on GPUs with Tensor Fusion.
+
+        This test will crash badly if used with an MPI implementation that does
+        not support GPU memory transfers directly, as it will call MPI_Send on
+        a GPU data pointer."""
+        # Only do this test if there are GPUs available.
+        if not tf.test.is_gpu_available(cuda_only=True):
+            self.skipTest(("No GPUs available"))
+
+        if os.environ.get('HOROVOD_MIXED_INSTALL'):
+            # Skip if compiled with CUDA but without HOROVOD_GPU_OPERATIONS.
+            self.skipTest("Not compiled with HOROVOD_GPU_OPERATIONS")
+
+        hvd.init()
+        local_rank = hvd.local_rank()
+        size = hvd.size()
+
+        dtypes = [tf.int32, tf.float32]
+        dims = [1, 2, 3]
+        tests = []
+        for dtype, dim in itertools.product(dtypes, dims):
+            with tf.device("/gpu:%d" % local_rank):
+                tensor = self.random_uniform(
+                    [17] * dim, -100, 100, dtype=dtype)
+                summed = hvd.allreduce(tensor, average=False)
+            multiplied = tensor * size
+            max_difference = tf.reduce_max(tf.abs(summed - multiplied))
+
+            # Threshold for floating point equality depends on number of
+            # ranks, since we're comparing against precise multiplication.
+            if size <= 3 or dtype in [tf.int32, tf.int64]:
+                threshold = 0
+            elif size < 10:
+                threshold = 1e-4
+            elif size < 15:
+                threshold = 5e-4
+            else:
+                self.skipTest("Horovod cluster too large for precise multiplication comparison")
+
+            test = max_difference <= threshold
+            tests.append(test)
+        self.assertTrue(self.evaluate(tf.reduce_all(tests)),
+                        "hvd.omni_allreduce produces incorrect results")
+
+    
+if _has_eager:
+    from tensorflow.python.framework.test_util import run_all_in_graph_and_eager_modes
+    run_all_in_graph_and_eager_modes(TensorFlowTests)
+
+if __name__ == '__main__':
+    tf.test.main()
diff --git a/test/test_torch_omni.py b/test/test_torch_omni.py
new file mode 100644
index 0000000..dfb0b01
--- /dev/null
+++ b/test/test_torch_omni.py
@@ -0,0 +1,238 @@
+# Copyright 2018 Uber Technologies, Inc. All Rights Reserved.
+# Modifications copyright (C) 2019 Intel Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+from distutils.version import LooseVersion
+
+import inspect
+import itertools
+import os
+import pytest
+import unittest
+import warnings
+
+from collections.abc import Iterable
+
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+import horovod.torch as hvd
+
+from common import mpi_env_rank_and_size, temppath
+
+_v2_api = LooseVersion(torch.__version__) >= LooseVersion('1.0.0')
+_fp16_supported = _v2_api
+
+ccl_supported_types = set([torch.CharTensor, torch.IntTensor,
+                           torch.LongTensor, torch.FloatTensor, 
+                           torch.DoubleTensor])
+
+
+class TorchTests(unittest.TestCase):
+    """
+    Tests for ops in horovod.torch.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(TorchTests, self).__init__(*args, **kwargs)
+        warnings.simplefilter('module')
+
+    def convert_cpu_fp16_to_fp32(self, *values):
+        # PyTorch doesn't support any CPU ops on FP16 tensors.
+        # In case we need to do ops, we will convert tensor to FP32 here.
+        result = []
+        for value in values:
+            if value.dtype in [torch.float16, torch.HalfTensor]:
+                result.append(value.float())
+            else:
+                result.append(value)
+        return result
+
+    def cast_and_place(self, tensor, dtype):
+        if dtype.is_cuda:
+            return tensor.cuda(hvd.local_rank()).type(dtype)
+        return tensor.type(dtype)
+
+    def filter_supported_types(self, types):
+        if 'CCL_ROOT' in os.environ:
+           types = [t for t in types if t in ccl_supported_types]
+        return types
+
+    def test_horovod_reinit(self):
+        """Test that Horovod can init -> shutdown -> init successfully."""
+        mpi_rank, _ = mpi_env_rank_and_size()
+        gloo_rank = int(os.getenv('HOROVOD_RANK', -1))
+
+        is_mpi = gloo_rank == -1
+        if is_mpi:
+            # Only applies for Gloo
+            self.skipTest("Gloo is not available")
+
+        hvd.init()
+        rank, size = hvd.rank(), hvd.size()
+
+        hvd.shutdown()
+        hvd.init()
+        rank2, size2 = hvd.rank(), hvd.size()
+
+        assert rank == rank2
+        assert size == size2
+
+    def test_horovod_rank(self):
+        """Test that the rank returned by hvd.rank() is correct."""
+        mpi_rank, _ = mpi_env_rank_and_size()
+        gloo_rank = int(os.getenv('HOROVOD_RANK', -1))
+
+        # The mpi rank does not match gloo rank, we need to figure which one
+        # we are using to run the test.
+        is_mpi = gloo_rank == -1
+        hvd.init()
+        rank = hvd.rank()
+
+        if is_mpi:
+            assert mpi_rank == rank
+        else:
+            assert gloo_rank == rank
+
+    def test_horovod_size(self):
+        """Test that the size returned by hvd.size() is correct."""
+        _, mpi_size = mpi_env_rank_and_size()
+        gloo_size = int(os.getenv('HOROVOD_SIZE', -1))
+
+        # The mpi size does not match gloo size, we need to figure which one
+        # we are using to run the test.
+        is_mpi = gloo_size == -1
+        hvd.init()
+        size = hvd.size()
+        if is_mpi:
+            assert mpi_size == size
+        else:
+            assert gloo_size == size
+
+    def test_horovod_allreduce(self):
+        """Test that the allreduce correctly sums 1D, 2D, 3D tensors."""
+        print("start to test_horovod_allreduce")
+        hvd.init()
+        size = hvd.size()
+
+        dtypes = [torch.cuda.IntTensor, torch.cuda.FloatTensor]
+        dims = [1, 2, 3]
+        for dtype, dim in itertools.product(dtypes, dims):
+            torch.manual_seed(1234)
+            tensor = torch.FloatTensor(*([17] * dim)).random_(-100, 100)
+            tensor = self.cast_and_place(tensor, dtype)
+            summed = hvd.allreduce(tensor, average=False)
+            tensor, summed = self.convert_cpu_fp16_to_fp32(tensor, summed)
+            multiplied = tensor * size
+            max_difference = summed.data.sub(multiplied).max()
+            print("tensor={}".format(tensor))
+            print("summed={}".format(summed))
+            print("multiplied={}".format(multiplied))
+            print("max_difference={}".format(max_difference))
+
+            # Threshold for floating point equality depends on number of
+            # ranks, since we're comparing against precise multiplication.
+            if size <= 3 or dtype in [torch.cuda.IntTensor]:
+                threshold = 0
+            elif size < 10:
+                threshold = 1e-4
+            elif size < 15:
+                threshold = 5e-4
+            else:
+                break
+
+            assert max_difference <= threshold, 'hvd.allreduce produces incorrect results'
+        print("finish to test_horovod_allreduce")
+
+    def test_horovod_allreduce_inplace(self):
+        """Test that the allreduce correctly sums 1D, 2D, 3D tensors."""
+        print("start to test_horovod_allreduce_inplace")
+        hvd.init()
+        size = hvd.size()
+        dtypes = [torch.cuda.IntTensor, torch.cuda.FloatTensor]
+        dims = [1, 2, 3]
+        for dtype, dim in itertools.product(dtypes, dims):
+            torch.manual_seed(1234)
+            tensor = torch.FloatTensor(*([17] * dim)).random_(-100, 100)
+            multiplied = self.cast_and_place(tensor * size, dtype)
+            tensor = self.cast_and_place(tensor, dtype)
+            hvd.allreduce_(tensor, average=False)
+            tensor, multiplied = self.convert_cpu_fp16_to_fp32(tensor, multiplied)
+            max_difference = tensor.sub(multiplied).max()
+            print("tensor={}".format(tensor))
+            print("multiplied={}".format(multiplied))
+            print("max_difference={}".format(max_difference))
+
+            # Threshold for floating point equality depends on number of
+            # ranks, since we're comparing against precise multiplication.
+            if size <= 3 or dtype in [torch.cuda.IntTensor]:
+                threshold = 0
+            elif size < 10:
+                threshold = 1e-4
+            elif size < 15:
+                threshold = 5e-4
+            else:
+                break
+
+            assert max_difference <= threshold, 'hvd.allreduce produces incorrect results'
+        print("finish to test_horovod_allreduce_inplace")
+    
+    def test_horovod_allreduce_async_fused(self):
+        """Test that the allreduce correctly sums 1D, 2D, 3D tensors
+        with Tensor Fusion."""
+        print("start to test_horovod_allreduce_async_fused")
+        hvd.init()
+        size = hvd.size()
+        dtypes = [torch.cuda.IntTensor, torch.cuda.FloatTensor]
+        dims = [1, 2, 3]
+        tests = []
+        is_hvd_poll_false_once = False
+        for dtype, dim in itertools.product(dtypes, dims):
+            torch.manual_seed(1234)
+            tensor = torch.FloatTensor(*([17] * dim)).random_(-100, 100)
+            tensor = self.cast_and_place(tensor, dtype)
+            handle = hvd.allreduce_async(tensor, average=False)
+            if not hvd.poll(handle):
+                is_hvd_poll_false_once = True
+            tensor, = self.convert_cpu_fp16_to_fp32(tensor)
+            multiplied = tensor * size
+            tests.append((dtype, multiplied, handle))
+
+        # Make sure it's an asynchronous operation.
+        assert is_hvd_poll_false_once, 'hvd.poll() always returns True, not an async op?'
+
+        for dtype, multiplied, handle in tests:
+            summed = hvd.synchronize(handle)
+            summed, = self.convert_cpu_fp16_to_fp32(summed)
+            max_difference = summed.sub(multiplied).max()
+
+            # Threshold for floating point equality depends on number of
+            # ranks, since we're comparing against precise multiplication.
+            if size <= 3 or dtype in [torch.cuda.IntTensor]:
+                threshold = 0
+            elif size < 10:
+                threshold = 1e-4
+            elif size < 15:
+                threshold = 5e-4
+            else:
+                break
+
+            assert max_difference <= threshold, 'hvd.allreduce produces incorrect results'
+        print("finish to test_horovod_allreduce_async_fused")
+
+if __name__ == "__main__":
+   unittest.main()
-- 
2.24.3 (Apple Git-128)

